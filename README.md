# Deceiving Autonomous Cars with Toxic Signs #

### Abstract ###
Recent studies show that the cutting edge deep neural networks are vulnerable to contradictory examples,
deriving from small magnitude perturbations added to the input. With the advent of self-driving machines,
the contradictory example, as we can imagine, can generate many complications: a car can interpret a signal
incorrectly and generate an accident. In our project, we want to analyze and test the problem,
showing that it is possible to generate specific perturbations to the input images to confuse the model and, in
some way, force the network prediction.

### Requirements ###
To test our code you can simply install a virtual environment on your machine ([Turorial](https://www.tensorflow.org/install/pip)). 
Then, you can run the following code to install all the necessary libraries:
```
python setup.py
```
### Team ###
* [Antonino Di Maggio](https://www.linkedin.com/in/antonino-di-maggio/) 
* [Leonardo Salvucci](https://www.linkedin.com/in/leonardo-salvucci/)  

### Useful links ###
Slide presentation: In progress <br/>
Paper: https://drive.google.com/open?id=1vDJjxpUsyqt0kizyF87VwtZb8NFrJxrr <br/>
