### Deceiving Autonomous Cars with Toxic Signs ###

# Abstract #
Recent studies show that the cutting edge deep neural networks are vulnerable to contradictory examples,
deriving from small magnitude perturbations added to the input. With the advent of self-driving machines,
the contradictory example, as we can imagine, can generate many complications: a car can interpret a signal
incorrectly and generate an accident. In the following report, we want to analyze and test the problem,
showing that it is possible to generate specific perturbations to the input images to confuse the model and, in
some way, force the network prediction.

